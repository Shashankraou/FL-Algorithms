# Decentralized Federated Learning - Code Explanation

## Overview

This code implements four decentralized federated learning algorithms that train a CNN on MNIST without a central server. Clients communicate only with their neighbors via gossip protocols.

---

## Code Structure

### 1. Imports and Setup

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
import copy
from tqdm.auto import tqdm
```

- `torch` - Core PyTorch for tensors and autograd
- `torch.nn` - Neural network layers and loss functions
- `torch.optim` - Optimizers (SGD)
- `torchvision` - Dataset loading (MNIST)
- `numpy` - Numerical operations (averaging accuracies)
- `matplotlib` - Plotting results
- `copy` - Deep copying model states for snapshots
- `tqdm` - Progress bars

```python
torch.manual_seed(42)
np.random.seed(42)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

Seeds ensure reproducibility. Device detection uses GPU if available.

---

### 2. Hyperparameters

```python
NUM_CLIENTS     = 5       # Number of participating nodes
BATCH_SIZE      = 128     # Mini-batch size for SGD
LOCAL_EPOCHS    = 5       # Training epochs per round
GLOBAL_ROUNDS   = 20      # Total communication rounds
LR              = 0.01    # Learning rate
MOMENTUM        = 0.9     # SGD momentum
WEIGHT_DECAY    = 1e-4    # L2 regularization
MU              = 0.01    # FedProx proximal term coefficient
MASK_SPARSITY   = 0.1     # Fraction of weights frozen in FedMask
TOPOLOGY        = "mesh"  # "mesh" or "ring"
GOSSIP_FREQUENCY = 3      # Gossip every N rounds
```

---

### 3. Neural Network Architecture

```python
class MNISTNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3)    # 28×28 → 26×26×32
        self.conv2 = nn.Conv2d(32, 64, 3)   # 26×26 → 24×24×64
        self.pool  = nn.MaxPool2d(2)         # 24×24 → 12×12×64
        self.fc1   = nn.Linear(9216, 128)    # Flatten → 128
        self.fc2   = nn.Linear(128, 10)      # 128 → 10 classes
```

**Architecture:**
- Input: 28×28 grayscale image
- Conv1: 1→32 channels, 3×3 kernel, ReLU → 26×26×32
- Conv2: 32→64 channels, 3×3 kernel, ReLU → 24×24×64
- MaxPool: 2×2 → 12×12×64
- Flatten: 64×12×12 = 9216 features
- FC1: 9216→128, ReLU
- FC2: 128→10 (logits for digits 0-9)

```python
def forward(self, x):
    x = torch.relu(self.conv1(x))
    x = torch.relu(self.conv2(x))
    x = self.pool(x)
    x = torch.flatten(x, 1)
    x = torch.relu(self.fc1(x))
    return self.fc2(x)
```

Forward pass applies layers sequentially. No softmax needed (handled by CrossEntropyLoss).

---

### 4. Data Preparation

```python
transform = transforms.Compose([
    transforms.ToTensor(),                    # PIL → Tensor [0,1]
    transforms.Normalize((0.1307,), (0.3081,))  # Standardize
])

train_dataset = torchvision.datasets.MNIST('./data', train=True, download=True, transform=transform)
test_dataset  = torchvision.datasets.MNIST('./data', train=False, download=True, transform=transform)
```

Downloads MNIST (60k train, 10k test). Transform converts images to tensors and normalizes using dataset statistics.

```python
data_per_client = len(train_dataset) // NUM_CLIENTS  # 60000 // 5 = 12000

client_loaders = []
for i in range(NUM_CLIENTS):
    subset = torch.utils.data.Subset(
        train_dataset,
        list(range(i * data_per_client, (i + 1) * data_per_client))
    )
    loader = torch.utils.data.DataLoader(subset, batch_size=BATCH_SIZE, shuffle=True)
    client_loaders.append(loader)
```

Splits training data into 5 equal partitions (IID split):
- Client 0: indices 0-11999
- Client 1: indices 12000-23999
- etc.

Each DataLoader serves shuffled batches of 128 samples.

```python
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)
```

Single shared test loader for evaluation.

---

### 5. Topology Creation

```python
def create_topology(num_clients, topology_type="mesh"):
    if topology_type == "mesh":
        graph = {i: [j for j in range(num_clients) if j != i] for i in range(num_clients)}
    elif topology_type == "ring":
        graph = {i: [(i + 1) % num_clients, (i - 1) % num_clients] for i in range(num_clients)}
    return graph
```

Returns adjacency list:
- **Mesh**: `{0: [1,2,3,4], 1: [0,2,3,4], ...}` - fully connected
- **Ring**: `{0: [1,4], 1: [2,0], ...}` - each node connects to 2 neighbors

---

### 6. Evaluation Function

```python
def evaluate_model(model):
    model.eval()
    correct = 0
    with torch.no_grad():
        for x, y in test_loader:
            x, y = x.to(device), y.to(device)
            preds = model(x).argmax(dim=1)
            correct += (preds == y).sum().item()
    return 100 * correct / len(test_dataset)
```

- `model.eval()` - Sets model to evaluation mode
- `torch.no_grad()` - Disables gradient computation (saves memory)
- `argmax(dim=1)` - Picks class with highest score
- Returns accuracy percentage

---

### 7. Gossip Averaging

```python
def gossip_average(client_models, topology):
    # Step 1: Snapshot all models BEFORE any updates
    snapshots = [copy.deepcopy(m.state_dict()) for m in client_models]
    
    # Step 2: Each client averages with neighbors
    for client_id, model in enumerate(client_models):
        neighbor_ids = topology[client_id]
        all_states   = [snapshots[client_id]] + [snapshots[nid] for nid in neighbor_ids]
        
        avg_state = {}
        for key in all_states[0].keys():
            # Stack tensors and take mean across clients
            avg_state[key] = torch.stack([s[key].float() for s in all_states]).mean(0)
        
        model.load_state_dict(avg_state)
```

**Critical details:**

1. **Snapshot first**: Prevents order-dependency bugs. Without snapshots, client 0's updated weights would contaminate client 1's averaging.

2. **For each client**:
   - Collect its snapshot + all neighbors' snapshots
   - For each parameter (conv1.weight, fc2.bias, etc.):
     - Stack all versions → shape becomes (num_neighbors+1, *param_shape)
     - Take mean along first dimension
     - Load averaged weights back

**Example (3 clients in mesh):**
```python
# Before gossip:
Client 0: weight = [1.0, 2.0, 3.0]
Client 1: weight = [4.0, 5.0, 6.0]
Client 2: weight = [7.0, 8.0, 9.0]

# After gossip (all average with everyone):
All clients: weight = [(1+4+7)/3, (2+5+8)/3, (3+6+9)/3]
           = [4.0, 5.0, 6.0]
```

---

## Algorithm Implementations

### Algorithm 1: D-FedAvg

```python
def train_dfedavg():
    client_models = [MNISTNet().to(device) for _ in range(NUM_CLIENTS)]
    accuracies = []
    
    for round_num in tqdm(range(1, GLOBAL_ROUNDS + 1)):
        # Local training
        for client_id, model in enumerate(client_models):
            model.train()
            optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
            
            for epoch in range(LOCAL_EPOCHS):
                for x, y in client_loaders[client_id]:
                    x, y = x.to(device), y.to(device)
                    optimizer.zero_grad()
                    loss = nn.CrossEntropyLoss()(model(x), y)
                    loss.backward()
                    optimizer.step()
        
        # Gossip every GOSSIP_FREQUENCY rounds
        if round_num % GOSSIP_FREQUENCY == 0:
            gossip_average(client_models, topology)
        
        # Evaluation
        avg_acc = np.mean([evaluate_model(m) for m in client_models])
        accuracies.append(avg_acc)
    
    return accuracies
```

**Flow:**
1. Initialize 5 independent models
2. Each round:
   - Each client trains on its local data for 5 epochs
   - Optimizer recreated each round (no persistent momentum)
   - Every 3 rounds: gossip (average weights with neighbors)
   - Evaluate all models and record average accuracy
3. Return accuracy history

---

### Algorithm 2: D-FedProx

```python
def train_dfedprox():
    client_models = [MNISTNet().to(device) for _ in range(NUM_CLIENTS)]
    accuracies = []
    
    for round_num in tqdm(range(1, GLOBAL_ROUNDS + 1)):
        # Compute global reference (consensus point)
        global_params = {}
        for key in client_models[0].state_dict().keys():
            global_params[key] = torch.stack([
                m.state_dict()[key].float() for m in client_models
            ]).mean(0).to(device)
        
        # Local training with proximal term
        for client_id, model in enumerate(client_models):
            model.train()
            optimizer = optim.SGD(model.parameters(), lr=LR)
            
            for epoch in range(LOCAL_EPOCHS):
                for x, y in client_loaders[client_id]:
                    x, y = x.to(device), y.to(device)
                    optimizer.zero_grad()
                    
                    # Standard loss
                    loss = nn.CrossEntropyLoss()(model(x), y)
                    
                    # Proximal term: penalize distance from consensus
                    prox_term = 0
                    for name, param in model.named_parameters():
                        prox_term += ((param - global_params[name]) ** 2).sum()
                    
                    total_loss = loss + (MU / 2) * prox_term
                    total_loss.backward()
                    optimizer.step()
        
        if round_num % GOSSIP_FREQUENCY == 0:
            gossip_average(client_models, topology)
        
        avg_acc = np.mean([evaluate_model(m) for m in client_models])
        accuracies.append(avg_acc)
    
    return accuracies
```

**Key difference:**

Before training, computes `global_params` = average of all client parameters.

During training: `total_loss = CE_loss + (μ/2) * ||weights - global_params||²`

The proximal term acts as a regularizer pulling clients toward consensus. `MU=0.01` controls strength.

**Math:**
```
For each weight w:
    prox_term += (w - w_global)²
    
Gradient of prox_term = 2(w - w_global)
This pulls w toward w_global
```

---

### Algorithm 3: D-FedPer

```python
def train_dfedper():
    client_models = [MNISTNet().to(device) for _ in range(NUM_CLIENTS)]
    accuracies = []
    
    for round_num in tqdm(range(1, GLOBAL_ROUNDS + 1)):
        # Local training (full model)
        for client_id, model in enumerate(client_models):
            model.train()
            optimizer = optim.SGD(model.parameters(), lr=LR)
            
            for epoch in range(LOCAL_EPOCHS):
                for x, y in client_loaders[client_id]:
                    x, y = x.to(device), y.to(device)
                    optimizer.zero_grad()
                    loss = nn.CrossEntropyLoss()(model(x), y)
                    loss.backward()
                    optimizer.step()
        
        # Gossip: share body only, keep personal head
        if round_num % GOSSIP_FREQUENCY == 0:
            # Save personal heads before gossip
            personal_heads = [copy.deepcopy(m.fc2.state_dict()) for m in client_models]
            
            snapshots = [copy.deepcopy(m.state_dict()) for m in client_models]
            
            for client_id, model in enumerate(client_models):
                neighbor_ids = topology[client_id]
                all_states = [snapshots[client_id]] + [snapshots[nid] for nid in neighbor_ids]
                
                avg_state = {}
                for key in all_states[0].keys():
                    if "fc2" in key:
                        # Personal head: keep own weights
                        avg_state[key] = snapshots[client_id][key]
                    else:
                        # Shared body: average with neighbors
                        avg_state[key] = torch.stack([s[key].float() for s in all_states]).mean(0)
                
                model.load_state_dict(avg_state)
                
                # Double-check head restoration
                model.fc2.load_state_dict(personal_heads[client_id])
        
        avg_acc = np.mean([evaluate_model(m) for m in client_models])
        accuracies.append(avg_acc)
    
    return accuracies
```

**Key difference:**

During gossip, only shared body (conv1, conv2, fc1) is averaged. Personal head (fc2) is excluded:

```python
if "fc2" in key:
    avg_state[key] = snapshots[client_id][key]  # Keep own
else:
    avg_state[key] = torch.stack([...]).mean(0)  # Average
```

This allows:
- Shared feature extraction (conv layers learn common patterns)
- Personal classification (each client's fc2 adapts to local data distribution)

---

### Algorithm 4: D-FedMask

```python
def train_dfedmask():
    client_models = [MNISTNet().to(device) for _ in range(NUM_CLIENTS)]
    
    # Create binary masks for each client
    masks = []
    for _ in range(NUM_CLIENTS):
        mask = {}
        for name, param in client_models[0].named_parameters():
            if "weight" in name:
                m = torch.ones_like(param)
                num_zero = int(param.numel() * MASK_SPARSITY)
                zero_indices = torch.randperm(param.numel())[:num_zero]
                m.view(-1)[zero_indices] = 0
                mask[name] = m.to(device)
            else:
                # Biases not masked
                mask[name] = torch.ones_like(param).to(device)
        masks.append(mask)
    
    accuracies = []
    
    for round_num in tqdm(range(1, GLOBAL_ROUNDS + 1)):
        # Local training with masked gradients
        for client_id, model in enumerate(client_models):
            model.train()
            optimizer = optim.SGD(model.parameters(), lr=LR)
            
            for epoch in range(LOCAL_EPOCHS):
                for x, y in client_loaders[client_id]:
                    x, y = x.to(device), y.to(device)
                    optimizer.zero_grad()
                    loss = nn.CrossEntropyLoss()(model(x), y)
                    loss.backward()
                    
                    # Zero out gradients for masked weights
                    for name, param in model.named_parameters():
                        if param.grad is not None:
                            param.grad *= masks[client_id][name]
                    
                    optimizer.step()
        
        if round_num % GOSSIP_FREQUENCY == 0:
            gossip_average(client_models, topology)
        
        avg_acc = np.mean([evaluate_model(m) for m in client_models])
        accuracies.append(avg_acc)
    
    return accuracies
```

**Key components:**

1. **Mask initialization**: For each client, create binary mask where 10% of weight values are 0, rest are 1. Biases always 1.

```python
m = torch.ones_like(param)              # All 1s
num_zero = int(param.numel() * 0.1)    # 10% to zero
zero_indices = torch.randperm(...)      # Random positions
m.view(-1)[zero_indices] = 0           # Set to 0
```

2. **Masked training**: After computing gradients, multiply by mask:

```python
param.grad *= masks[client_id][name]
```

This zeros gradients for masked weights → they never update → they stay at initialization.

3. **Gossip**: Averages all weights (including masked ones). Ideally should only average overlapping active weights, but simplified here.

**Result**: Each client trains a different sparse subnetwork.

---

## Main Execution

```python
results = {}
results["D-FedAvg"]  = train_dfedavg()
results["D-FedProx"] = train_dfedprox()
results["D-FedPer"]  = train_dfedper()
results["D-FedMask"] = train_dfedmask()
```

Runs all four algorithms sequentially and stores per-round accuracies.

---

## Plotting

```python
plt.figure(figsize=(14, 5))

# Left plot: accuracy curves
plt.subplot(1, 2, 1)
for (name, acc), color in zip(results.items(), colors):
    plt.plot(acc, marker='o', label=name, linewidth=2, markersize=4, color=color)
plt.axhline(95, linestyle="--", color='red', alpha=0.4)
plt.xlabel("Rounds")
plt.ylabel("Accuracy (%)")
plt.title("Decentralized FL Performance")
plt.legend()
plt.grid(alpha=0.3)

# Right plot: final accuracy bars
plt.subplot(1, 2, 2)
final_accs = [acc[-1] for acc in results.values()]
bars = plt.bar(results.keys(), final_accs, color=colors, alpha=0.7)
for bar, acc in zip(bars, final_accs):
    plt.text(bar.get_x() + bar.get_width() / 2., bar.get_height() + 1, 
             f'{acc:.1f}%', ha='center')
plt.ylabel("Final Accuracy (%)")
plt.title("Final Performance Comparison")

plt.tight_layout()
plt.show()
```

Creates two subplots:
1. Line chart showing accuracy vs. round for each algorithm
2. Bar chart comparing final accuracies with values labeled

---

## Key Implementation Details

### Why `copy.deepcopy()` in gossip?

Without deep copy, all clients would reference the same tensor memory:
```python
# BAD:
state = model.state_dict()  # Still points to model's memory
model.fc1.weight.data = new_value  # Changes state too!

# GOOD:
state = copy.deepcopy(model.state_dict())  # Independent copy
model.fc1.weight.data = new_value  # state unchanged
```

### Why recreate optimizer each round?

Simplification. Persistent optimizers would maintain momentum/Adam state across rounds. Here momentum resets each round.

### Why IID data split?

Easiest case. Real FL typically has non-IID data where each client has skewed distributions. That's where D-FedPer excels.

### Why gossip every 3 rounds?

Trade-off between communication cost and convergence speed. More frequent = faster but more expensive.

